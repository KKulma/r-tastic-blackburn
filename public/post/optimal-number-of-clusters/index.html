<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.47.1" />

  <title>Determining the optimal number of clusters in your dataset &middot; r-tastic</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

 
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/androidstudio.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="/">Kasia Kulma</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/contact/"><i class='fa fa-phone fa-fw'></i>Contact</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2018. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Determining the optimal number of clusters in your dataset</h1>
  <h2></h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>24 Apr 2017</time>
  </div>

  

  
  
  
  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="/tags/clustering">clustering</a>&nbsp;&#47;
    
      <a class="post-taxonomy-tag" href="/tags/best-practice">best practice</a>
    
  </div>
  
  

</div>

  <p>Recently, I worked a bit with <strong>cluster analysis</strong>: the common method in <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a> that uses datasets without labeled responses to draw inferences. I wanted to put my notes together and write it all down before I forget it, thus the blog post. For the start, I’ll tackle multiple approaches to how to determine the number of clusters in your data.</p>
<div id="quick-intro" class="section level3">
<h3>QUICK INTRO</h3>
<p>Clustering algorithms aim to establish a structure of your data and assign a cluster/segment to each datapoint based on the input data. Most popular clustering learners (e.g. k-means) expect you to specify the number of segments to be found. However, clusters as such don’t exist in firm reality, so more often than not you don’t know what is the optimal number of clusters for a given dataset.</p>
<p><strong>There are multiple methods you can use in order to determine what is the optimal number of segments for your data and that is what I’m going to review in this post.</strong></p>
<p>I’m not going to go into details of each algorithm’s workings, but will provide references for you to follow up if you want to find out more.</p>
<p>Keep in mind that this list is not complete (for more complete list check out this <a href="http://stackoverflow.com/a/15376462">excellent StackOverflow answer</a> ) and that not all approaches will return the same answer. In such case, my advise would be: try several methods - the more consistency between different approaches, the better - pick the most commonly returned number of clusters, evaluate their consistency and structure (I’ll cover how to do it in the next post) and check if they make sense from the real world point of view.</p>
<p>So there we go!</p>
</div>
<div id="wine-dataset" class="section level3">
<h3>WINE DATASET</h3>
<p>For this exercise, I’ll use a popular <code>wine</code> datasets that you can find built into R under several packages (e.g. <code>gclus</code>, <code>HDclassif</code> or <code>rattle</code> packages). The full description of the dataset you can find <a href="https://archive.ics.uci.edu/ml/datasets/Wine">here</a>, but essentially if contains results of a chemical analysis of 3 different types of wines grown in the same region in Italy. I’m <strong>guessing</strong> that the three types of wine (described by <code>Class</code> variable in the dataset) mean white, red and rose, but I couldn’t find anything to confirm it or to disclose which Class in the data corresponds to which wine type… Anyway! Let’s load the data and have a quick look at it:</p>
<pre class="r"><code>## loading packages
library(dplyr)
library(knitr)
library(gclus)

# loading data
data(wine)
head(wine) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Class</th>
<th align="right">Alcohol</th>
<th align="right">Malic</th>
<th align="right">Ash</th>
<th align="right">Alcalinity</th>
<th align="right">Magnesium</th>
<th align="right">Phenols</th>
<th align="right">Flavanoids</th>
<th align="right">Nonflavanoid</th>
<th align="right">Proanthocyanins</th>
<th align="right">Intensity</th>
<th align="right">Hue</th>
<th align="right">OD280</th>
<th align="right">Proline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">14.23</td>
<td align="right">1.71</td>
<td align="right">2.43</td>
<td align="right">15.6</td>
<td align="right">127</td>
<td align="right">2.80</td>
<td align="right">3.06</td>
<td align="right">0.28</td>
<td align="right">2.29</td>
<td align="right">5.64</td>
<td align="right">1.04</td>
<td align="right">3.92</td>
<td align="right">1065</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">13.20</td>
<td align="right">1.78</td>
<td align="right">2.14</td>
<td align="right">11.2</td>
<td align="right">100</td>
<td align="right">2.65</td>
<td align="right">2.76</td>
<td align="right">0.26</td>
<td align="right">1.28</td>
<td align="right">4.38</td>
<td align="right">1.05</td>
<td align="right">3.40</td>
<td align="right">1050</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">13.16</td>
<td align="right">2.36</td>
<td align="right">2.67</td>
<td align="right">18.6</td>
<td align="right">101</td>
<td align="right">2.80</td>
<td align="right">3.24</td>
<td align="right">0.30</td>
<td align="right">2.81</td>
<td align="right">5.68</td>
<td align="right">1.03</td>
<td align="right">3.17</td>
<td align="right">1185</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">14.37</td>
<td align="right">1.95</td>
<td align="right">2.50</td>
<td align="right">16.8</td>
<td align="right">113</td>
<td align="right">3.85</td>
<td align="right">3.49</td>
<td align="right">0.24</td>
<td align="right">2.18</td>
<td align="right">7.80</td>
<td align="right">0.86</td>
<td align="right">3.45</td>
<td align="right">1480</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">13.24</td>
<td align="right">2.59</td>
<td align="right">2.87</td>
<td align="right">21.0</td>
<td align="right">118</td>
<td align="right">2.80</td>
<td align="right">2.69</td>
<td align="right">0.39</td>
<td align="right">1.82</td>
<td align="right">4.32</td>
<td align="right">1.04</td>
<td align="right">2.93</td>
<td align="right">735</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">14.20</td>
<td align="right">1.76</td>
<td align="right">2.45</td>
<td align="right">15.2</td>
<td align="right">112</td>
<td align="right">3.27</td>
<td align="right">3.39</td>
<td align="right">0.34</td>
<td align="right">1.97</td>
<td align="right">6.75</td>
<td align="right">1.05</td>
<td align="right">2.85</td>
<td align="right">1450</td>
</tr>
</tbody>
</table>
<pre class="r"><code>table(wine$Class)</code></pre>
<pre><code>## 
##  1  2  3 
## 59 71 48</code></pre>
<p><br></p>
<p>Next, I’ll remove the <code>Class</code> variable, as I don’t want it to affect clustering, and scale the data.</p>
<pre class="r"><code>scaled_wine &lt;- scale(wine) %&gt;% as.data.frame()

scaled_wine2 &lt;- scaled_wine[-1]

head(scaled_wine2) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Alcohol</th>
<th align="right">Malic</th>
<th align="right">Ash</th>
<th align="right">Alcalinity</th>
<th align="right">Magnesium</th>
<th align="right">Phenols</th>
<th align="right">Flavanoids</th>
<th align="right">Nonflavanoid</th>
<th align="right">Proanthocyanins</th>
<th align="right">Intensity</th>
<th align="right">Hue</th>
<th align="right">OD280</th>
<th align="right">Proline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.5143408</td>
<td align="right">-0.5606682</td>
<td align="right">0.2313998</td>
<td align="right">-1.1663032</td>
<td align="right">1.9085215</td>
<td align="right">0.8067217</td>
<td align="right">1.0319081</td>
<td align="right">-0.6577078</td>
<td align="right">1.2214385</td>
<td align="right">0.2510088</td>
<td align="right">0.3610679</td>
<td align="right">1.8427215</td>
<td align="right">1.0101594</td>
</tr>
<tr class="even">
<td align="right">0.2455968</td>
<td align="right">-0.4980086</td>
<td align="right">-0.8256672</td>
<td align="right">-2.4838405</td>
<td align="right">0.0180940</td>
<td align="right">0.5670481</td>
<td align="right">0.7315653</td>
<td align="right">-0.8184106</td>
<td align="right">-0.5431887</td>
<td align="right">-0.2924962</td>
<td align="right">0.4048188</td>
<td align="right">1.1103172</td>
<td align="right">0.9625263</td>
</tr>
<tr class="odd">
<td align="right">0.1963252</td>
<td align="right">0.0211715</td>
<td align="right">1.1062139</td>
<td align="right">-0.2679823</td>
<td align="right">0.0881098</td>
<td align="right">0.8067217</td>
<td align="right">1.2121137</td>
<td align="right">-0.4970050</td>
<td align="right">2.1299594</td>
<td align="right">0.2682629</td>
<td align="right">0.3173170</td>
<td align="right">0.7863692</td>
<td align="right">1.3912237</td>
</tr>
<tr class="even">
<td align="right">1.6867914</td>
<td align="right">-0.3458351</td>
<td align="right">0.4865539</td>
<td align="right">-0.8069748</td>
<td align="right">0.9282998</td>
<td align="right">2.4844372</td>
<td align="right">1.4623994</td>
<td align="right">-0.9791134</td>
<td align="right">1.0292513</td>
<td align="right">1.1827317</td>
<td align="right">-0.4264485</td>
<td align="right">1.1807407</td>
<td align="right">2.3280068</td>
</tr>
<tr class="odd">
<td align="right">0.2948684</td>
<td align="right">0.2270533</td>
<td align="right">1.8352256</td>
<td align="right">0.4506745</td>
<td align="right">1.2783790</td>
<td align="right">0.8067217</td>
<td align="right">0.6614853</td>
<td align="right">0.2261576</td>
<td align="right">0.4002753</td>
<td align="right">-0.3183774</td>
<td align="right">0.3610679</td>
<td align="right">0.4483365</td>
<td align="right">-0.0377675</td>
</tr>
<tr class="even">
<td align="right">1.4773871</td>
<td align="right">-0.5159113</td>
<td align="right">0.3043010</td>
<td align="right">-1.2860793</td>
<td align="right">0.8582840</td>
<td align="right">1.5576991</td>
<td align="right">1.3622851</td>
<td align="right">-0.1755994</td>
<td align="right">0.6623487</td>
<td align="right">0.7298108</td>
<td align="right">0.4048188</td>
<td align="right">0.3356589</td>
<td align="right">2.2327407</td>
</tr>
</tbody>
</table>
<p><br> Keep in mind that many presented algorithms rely on random starts (e.g. k-means), so I set a seed wherever I can so that we have reproducible examples.</p>
<p>Now we can start clustering extravaganza!</p>
<p><br></p>
</div>
<div id="determining-the-optimal-number-of-clusters" class="section level3">
<h3>DETERMINING THE OPTIMAL NUMBER OF CLUSTERS</h3>
<div id="elbow-method" class="section level4">
<h4>1. ELBOW METHOD</h4>
<p>In short, the elbow method maps the within-cluster sum of squares onto the number of possible clusters. As a rule of thumb, you pick the number for which you see a significant decrease in the within-cluster dissimilarity, or so called ‘elbow’. You can find more details <a href="https://www.coursera.org/learn/machine-learning/lecture/Ks0E9/choosing-the-number-of-clusters">here</a> or <a href="https://rstudio-pubs-static.s3.amazonaws.com/92318_20357e6dd99742eb90232c60c626fa90.html">here</a>.</p>
<p>Let’s see what happens when we apply the Elbow Method to our dataset:</p>
<pre class="r"><code>set.seed(13)

wss &lt;- (nrow(scaled_wine2)-1)*sum(apply(scaled_wine2,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(scaled_wine2,
                                     centers=i)$withinss)
plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;,
     ylab=&quot;Within groups sum of squares&quot;)</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>In this example, we see an ‘elbow’ at 3 clusters. By adding more clusters than that we get relatively smaller gains. So… 3 clusters!</p>
<p><br></p>
</div>
<div id="silhouette-analysis" class="section level4">
<h4>2. SILHOUETTE ANALYSIS</h4>
<p>The silhouette plots display a measure of how close each point in one cluster is to points in the neighboring clusters. This measure ranges from -1 to 1, where 1 means that points are very close to their own cluster and far from other clusters, whereas -1 indicates that points are close to the neighbouring clusters. You’ll find more details about silhouette analysis <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">here</a> and <a href="https://kapilddatascience.wordpress.com/2015/11/10/using-silhouette-analysis-for-selecting-the-number-of-cluster-for-k-means-clustering/">here</a>.</p>
<p>When we run the silhouette analysis on our wine dataset, we get the following graphs with indication on the optimal number of clusters:</p>
<pre class="r"><code>library(fpc)

set.seed(13)

pamk.best2 &lt;- pamk(scaled_wine2)
cat(&quot;number of clusters estimated by optimum average silhouette width:&quot;, pamk.best2$nc, &quot;\n&quot;)</code></pre>
<pre><code>## number of clusters estimated by optimum average silhouette width: 3</code></pre>
<pre class="r"><code>plot(pam(scaled_wine2, pamk.best2$nc))</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-6-1.png" width="672" /><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p><br></p>
<p>Thus, it’s 3 clusters again.</p>
<p><br></p>
</div>
<div id="gap-statistics" class="section level4">
<h4>3. GAP STATISTICS</h4>
<p>Gap statistic is a goodness of clustering measure, where for each hypothetical number of clusters k, it compares two functions: log of within-cluster sum of squares (wss) with its expectation under the null reference distribution of the data. In essence, it standardizes wss. It chooses the value where the log(wss) is the farthest below the reference curve, ergo the gap statistic is maximum. Pretty neat!</p>
<p>If you want to know more, check out <a href="https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjen9q-7rjTAhUeM8AKHcijBOEQFghpMAc&amp;url=https%3A%2F%2Flabs.genetics.ucla.edu%2Fhorvath%2FBiostat278%2FGapStatistics.ppt&amp;usg=AFQjCNFaWjeeR1F2UpJIskxYaBFt0eKZKg&amp;sig2=sQflpWNvc8BPu7ovHLyKuQ">this presentation</a> for a very good explanation and visualization of what gap statistic is.</p>
<p>Also, for even more details, see <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://web.stanford.edu/~hastie/Papers/gap.pdf">the original paper</a> where the gap statistic was first described, it’s like a good novel!</p>
<p>In the case of our wine dataset, gap analysis picks (again!) 3 cluster as an optimum:</p>
<pre class="r"><code>library(cluster)

set.seed(13)

clusGap(scaled_wine2, kmeans, 10, B = 100, verbose = interactive())</code></pre>
<pre><code>## Clustering Gap statistic [&quot;clusGap&quot;] from call:
## clusGap(x = scaled_wine2, FUNcluster = kmeans, K.max = 10, B = 100,     verbose = interactive())
## B=100 simulated reference sets, k = 1..10; spaceH0=&quot;scaledPCA&quot;
##  --&gt; Number of clusters (method &#39;firstSEmax&#39;, SE.factor=1): 3
##           logW   E.logW       gap     SE.sim
##  [1,] 5.377557 5.864072 0.4865153 0.01261541
##  [2,] 5.203502 5.757547 0.5540456 0.01356018
##  [3,] 5.066921 5.697040 0.6301193 0.01404664
##  [4,] 5.033404 5.651970 0.6185658 0.01410140
##  [5,] 4.992739 5.616114 0.6233751 0.01342023
##  [6,] 4.955442 5.584673 0.6292313 0.01304916
##  [7,] 4.933420 5.556602 0.6231820 0.01250233
##  [8,] 4.908585 5.531523 0.6229379 0.01299415
##  [9,] 4.884336 5.508464 0.6241281 0.01209969
## [10,] 4.858595 5.488182 0.6295870 0.01336910</code></pre>
<p><br></p>
</div>
<div id="hierarchical-clustering" class="section level4">
<h4>4. HIERARCHICAL CLUSTERING</h4>
<p>Hierarchical clustering is usually used to better understand the structure and relationships in your data and based on them you decide what number of clusters seems appropriate for your purpose. The way the algorithm works is very well described <a href="http://www.analytictech.com/networks/hiclus.htm">here</a> and <a href="http://www.saedsayad.com/clustering_hierarchical.htm">here</a>.</p>
<p>How to choose the optimal number of clusters based on the output of this analysis, the dendogram? As a rule of thumb, look for the clusters with the longest ‘branches’, the shorter they are, the more similar they are to following ‘twigs’ and ‘leaves’. But keep in mind that, as always, the optimal number will also depend on the context, expert knowledge, application, etc.</p>
<p>In our case, the solution is rather blurred:</p>
<pre class="r"><code>set.seed(13)

wine_dist2 &lt;- dist(as.matrix(scaled_wine2))   # find distance matrix
plot(hclust(wine_dist2))</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>Personally, I would go for something between 2 and 4 clusters. What do you think? :-)</p>
<p><br></p>
</div>
<div id="calinsky-criterion" class="section level4">
<h4>5. CALINSKY CRITERION</h4>
<p>yet another method evaluating optimal number of clusters based on within- and between-cluster distances. For more details, see <a href="http://ethen8181.github.io/machine-learning/clustering_old/clustering/clustering.html">here</a>.</p>
<p>In the example below we set the number of clusters to test between 1 and 10 and we iterated the test 1000 times.</p>
<pre class="r"><code>library(vegan)

set.seed(13)

cal_fit2 &lt;- cascadeKM(scaled_wine2, 1, 10, iter = 1000)
plot(cal_fit2, sortg = TRUE, grpmts.plot = TRUE)</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>calinski.best2 &lt;- as.numeric(which.max(cal_fit2$results[2,]))
cat(&quot;Calinski criterion optimal number of clusters:&quot;, calinski.best2, &quot;\n&quot;)</code></pre>
<pre><code>## Calinski criterion optimal number of clusters: 3</code></pre>
<p><br> Again, 3 clusters!</p>
<p><br></p>
</div>
<div id="bayesian-information-criterion-for-expectation---maximization" class="section level4">
<h4>6. BAYESIAN INFORMATION CRITERION FOR EXPECTATION - MAXIMIZATION</h4>
<p>This time in order to determine the optimal number of cluters we simultaneously run and compare several probabilistic models and pick the best one based on their BIC (Bayesian Information Criterion) score. For more details, make sure to read <a href="http://rstudio-pubs-static.s3.amazonaws.com/154174_78c021bc71ab42f8add0b2966938a3b8.html">this</a> and <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.8739">this</a>.</p>
<p>In the example below, we set the search of optimal number of clusters to be between 1 and 20.</p>
<pre class="r"><code>library(mclust)

set.seed(13)

d_clust2 &lt;- Mclust(as.matrix(scaled_wine2), G=1:20)
m.best2 &lt;- dim(d_clust2$z)[2]

cat(&quot;model-based optimal number of clusters:&quot;, m.best2, &quot;\n&quot;)</code></pre>
<pre><code>## model-based optimal number of clusters: 3</code></pre>
<pre class="r"><code>plot(d_clust2)</code></pre>
<div class="figure">
<img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/BIC-1.png" />

</div>
<div class="figure">
<img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/BIC-2.png" />

</div>
<div class="figure">
<img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/BIC-3.png" />

</div>
<div class="figure">
<img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/BIC-4.png" />

</div>
<p><br></p>
<p>And this time, the optimal number of clusters seems to be 4.</p>
<p><br></p>
</div>
<div id="affinity-propagation-clustering" class="section level4">
<h4>7. AFFINITY PROPAGATION CLUSTERING</h4>
<p>I’ll be quite honest with you: I’ve struggled to get my head around what affinity propagation is about! I understood that it clusters data by identifying a subset of representative examples, so called <strong>exemplars</strong>. And it does so by considering all data points as potential exemplars, but the rest is magic to me..</p>
<blockquote>
<blockquote>
<p><span class="math display">\[and\]</span> exchanging real-valued messages between data points until a high-quality set of exemplars and corresponding clusters gradually emerges.</p>
</blockquote>
</blockquote>
<p><em>(according to <a href="www.cs.columbia.edu/~delbert/docs/DDueck-thesis_small.pdf">Delbert Dueck</a>)</em></p>
<p>So if you have a better explanation or better materials.. send them over!</p>
<p>In our particular case, this algorithm identified</p>
<pre class="r"><code>library(apcluster)

set.seed(13)

d.apclus2 &lt;- apcluster(negDistMat(r=2), scaled_wine2)
cat(&quot;affinity propogation optimal number of clusters:&quot;, length(d.apclus2@clusters), &quot;\n&quot;)</code></pre>
<pre><code>## affinity propogation optimal number of clusters: 15</code></pre>
<pre class="r"><code>heatmap(d.apclus2)</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>plot(d.apclus2, scaled_wine2)</code></pre>
<p><img src="/post/2017-04-24-determining-the-optimal-number-of-clusters-in-your-datasets_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>
<div id="summary" class="section level3">
<h3>SUMMARY</h3>
<p>At least 4 (5, if including hierarchical clustering results) out of 7 tested algorithms suggested 3 as an optimal number of clusters for our data, which is a reasonable way forward, given that the analysed data contained information for 3 classes of wines ;-) In the next post I’ll evaluate those clusters in terms of their stability: how dissolved they are or how often they get recovered.</p>
</div>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="/post/scraping-online-table-with-info-on-r-datasets/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="/post/scraping-online-table-with-info-on-r-datasets/">Scraping online table with info on R datasets</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="/post/cluster-validation-in-unsupervised-machine-learning/">Cluster Validation In Unsupervised Machine Learning</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="/post/cluster-validation-in-unsupervised-machine-learning/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  
<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'https-r-tastic-co-uk';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>

</div>
</div>
<script src="/js/ui.js"></script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92539076-2', 'auto');
  ga('send', 'pageview');

</script>





</body>
</html>

