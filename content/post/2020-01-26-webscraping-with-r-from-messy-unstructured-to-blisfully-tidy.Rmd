---
title: Webscraping with R - from messy & unstructured to blisfully tidy
author: Kasia Kulma
date: '2020-01-26'
slug: from-messy-to-tidy
categories:
  - r
tags:
  - web scraping
  - data wrangling
  - tidyverse
description: ''
topics: []
---


I stopped caring about the estimates that ocassionally **ride** the internet about how much time data scientists spend on data wrangling VS modelling. The answer is: *probably* a lot and likely more than originally planned  (*probably* indicating here dependency on the state and richness of input data and the intended application for it, ekhem). 

Still, the right tools can go a long way in achieving a desired result in the time frame that can surprise even the most optiomistic of us. Needless to say, R is an excellent example of that right tool :).

## Goal

I want to be able to analyse the data on annual CO2 emissions per person for 130 nations worldwide published by Food and Agriculture Organization of the United Nations (FAO), that are nicely summarised on [nu3 website](https://www.nu3.de/blogs/nutrition/food-carbon-footprint-index-2018).


## Challenge

The data resides in the HTML table that has notoriously messy headers. Look and weep:

![](/post/2020-01-26-webscraping-with-r-from-messy-unstructured-to-blisfully-tidy_files/messy-html-table.PNG){width=800px}

But fear not! There's nothing that R can't fix in a blink of an eye. And here's how:

## Solution

First, load a handful of classic R packages:

- `{rvest}` for web-scraping
- `{dplyr}` for data-wrangling
- `{tidyr}` for data transformation
- `{stringr}` for string manipulation
- `{janitor}` for clean headers that your OCD will love you for


```{r load, message=FALSE, warning=FALSE}
library(rvest)
library(dplyr)
library(tidyr)
library(stringr)
library(janitor)
```
Next, scrape the content of the website and extract the HTML table:

```{r scrape}
url <- "https://www.nu3.de/blogs/nutrition/food-carbon-footprint-index-2018"

# scrape the website 
url_html <- read_html(url) 

# extract the HTML table
whole_table <- url_html %>%  
  html_nodes('table') %>% 
  html_table(fill = TRUE) %>% 
  .[[1]]

str(whole_table)
```

The state of column names can be summarised in only one word here: messmessmessmessmessmessmessmess....!! 

Before we tidy them up, let's scrap the headers and isolate just the content of the table:

```{r tidy_content}
# tidying the table
table_content <- whole_table %>% 
  select(-X1) %>% # remove redundant column
  filter(!dplyr::row_number() %in% 1:3) # remove redundant rows

head(table_content)
```

A bit better. It's worth noting that we're dealing with 26 variables that need column names. Now, how can we extract these?

When inspecting the HTML code behind the table (**Right Click => Inspect => Navigate to the part of the code that highlights the table headers**), you will notice that all the "icon headers" belong to `thead-icon` class that has an attribute `title` with the very column names that we're interested in.   

![](/post/2020-01-26-webscraping-with-r-from-messy-unstructured-to-blisfully-tidy_files/nodes&attributes.gif){width=800px}


`{rvest}` package makes it super easy to extract these using `html_nodes()` and `html_attr()` functions:

```{r raw_header3}
raw_headers <- url_html %>% 
  html_nodes(".thead-icon") %>% 
  html_attr('title') 

raw_headers[1:35]
```

As you can see, this way we extracted a few additional titles, but it's easy to select the correct ones from here. First, let's get the bottom-most names that refer to the actual column names: 

```{r tidy_header3}
tidy_bottom_header <- raw_headers[28:length(raw_headers)]
tidy_bottom_header[1:10]
```

Then, the middle ones:

```{r}
raw_middle_header <- raw_headers[17:27]
raw_middle_header
```

And finally, let's organise them in a way that the column names reflect the variable order: 

```{r}
tidy_headers <- c(
  rep(raw_middle_header[1:7], each = 2),
  "animal_total",
  rep(raw_middle_header[8:length(raw_middle_header)], each = 2),
  "non_animal_total",
  "country_total")

tidy_headers
```

I must say, this does the job but it is the part of the code that I'm least happy with, as it requires a lot of manual effort and thus is prone to errors. If you know better ways how to abstract/automate it, feel free to share it in the comments below!

Ok, over the first hurdle! Now, we want to retain the information about both, the type of food that the value refers too, as well as the metric that it describes (consumption or CO2 emmissions). Let's combine the two and make them column names:

```{r}
combined_colnames <- paste(tidy_headers, tidy_bottom_header, sep = ';')
colnames(table_content) <- c("Country", combined_colnames)
glimpse(table_content[, 1:10])
```

I know what you're thinking - this is faaaar from perfect (or pretty, for that matter), but we're only 2 lines of code away to having something much more digestable. Long live `{tidyr}` !!

```{r long_table}
long_table <- table_content %>%
  # make column names observations of Category variable 
  tidyr::pivot_longer(cols = -Country, names_to = "Category", values_to = "Values") %>%
  # separate food-related information from the metric
  tidyr::separate(col = Category, into = c("Food Category", "Metric"), sep = ';')

glimpse(long_table)
```

Almost there! I'm still not happy with the `Metric` variable that ideally should be split into two separate columns for an easier comparison of consumption and CO2 emmissions between the countries:

```{r tidy_table}
tidy_table <- long_table %>%
    tidyr::pivot_wider(names_from = Metric, values_from = Values) %>% 
    janitor::clean_names('snake')

glimpse(tidy_table)
```

Great stuff. Now we only need some final touches renaming the long column names and removing the instances of summarised data - these will be easy to calculate, if we need to, based on the final dataset: 

```{r final_table}
final_table <- tidy_table %>% 
  rename(consumption = 3,
          co2_emmission = 4) %>% 
  filter(!stringr::str_detect(food_category, "total"))

head(final_table, 20)
```


Voila! We've got what we wanted with a few lines of code and basic knowledge of HTML. The whole data-wrangling process took only a fraction of what I expected it to take and now the data is tidy and 'analysis-ready'.  

Do you know any examples of tricky-to-tidy data? Make sure to share your knowledge - and frustrations, why not!
